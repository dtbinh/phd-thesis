\section{Related Work}

Designing controllers for physically simulated biped characters is a
challenging problem due to its nonlinear dynamics and under-actuated
control. Earlier work demonstrated the potential of physics-based
character animation by simulating a variety of human motor skills
using manually constructed controllers
\revised{
\cite{Hodgins:1995:AHA,Wooten:1998:Phd,Faloutsos:2001:CCF}. 
}
%% \sehoon{Wooten is added}
The process of design
controllers required tedious parameter tuning, but the results were
compelling and inspiring. Later, researchers simplified the design
process by developing more general control principles 
resulting in much more robust locomotion controllers
\revtwo{\cite{Yin:2007:SIM,Coros:2010:GBW,Lee:2010:DDB,Lasa:2010:FLC}.}
The combination of PD servos and a
balance control strategy, which regulates the center of mass and
global orientation, has proven very effective in performing walking in
various environments with disturbances. We use similar underlying
controllers in this work: PD servos for controlling joint
configurations and Jacobian transpose control \cite{Sunada:1994:ACJ}
for regulating the center of mass. However, the parameters for these
low-level controllers, including the gains, target joint angles, and
desired virtual forces, are all determined automatically without any
user intervention.

Optimizing control parameters is a widely used approach in
biomechanics and computer animation. Due to the large number of
low-level control parameters, most previous work applied domain
knowledge \cite{Wang:2009:OWC,Wang:2010:OWC,Wang:2012:OLC}, used task
goals \cite{Yin:2008:CMA,Wu:2010:TBL,Liu:2012:TRC}, or formulated
smaller horizons \cite{Sok:2007:SBB} to reduce the problem
domain. Similarly, our work reduces the domain by using prior
knowledge to define a set of control rigs. These control rigs are
intuitively associated with human-readable instructions and are
general enough to be reused for new motor tasks. To solve the optimization
problem, sampling-based methods are usually preferred over
gradient-based methods due to the discontinuous nature of the
problem. Among different sampling-based optimization methods,
Covariance Matrix Adaptation Evolution Strategy \cite{Hansen:2004:CMA}
was frequently applied in computer animation in the recent years
because it converges relatively fast for high-dimensional problems. In
most cases, however, using CMA to optimize control parameters still
requires hours or even days of computation. To provide a more
interactive controller design framework, we introduce Covariance
Matrix Adaptation with Classification, a new algorithm, which speeds up
the standard CMA significantly.

\ignorethis{
Optimization has been applied in concert with feedback control to
track reference trajectories. Realistic locomotion can be generated by
a linear quadratic regulator \cite{daSilva:2008:ISS,Kwon:2010:CSH}, by a
nonlinear quadratic regulator \cite{Muico:2009:CNC}, or by linearizing the
first-order optimality condition \cite{Ye:2010:OFC}. Using reference
trajectories improves the realism of the motion, but its ability to
adapt to new situations is usually limited. Our work does not require
any reference motion, but it depends on the user to provide
appropriate guidance to achieve high-fidelity motion. Furthermore, our
framework incorporates the controller developer's intention
progressively and interactively instead of formulating a single
objective function upfront.
}

Controller generalization is also a challenging and important research
problem. Many researchers proposed various ways to combine or
concatenate existing controllers for new tasks. The common difficulty
of this problem is that the control parameters cannot be trivially
interpolated or blended. da Silva \etal demonstrated that optimal
controllers can be interpolated to create another optimal controller
for a new task using linear Bellman combination
\cite{daSilva:2009:LBC}. Similarly, Muico \etal proposed to track
multiple trajectories in parallel and automatically reweight different
actions to generate appropriate control force
\cite{Muico:2011:CCP}. Besides linear Bellman combination, Faloutsos
\etal \shortcite{Faloutsos:2001:CCF} developed a supervised learning
method to predict whether a controller can successfully handle the
transition between motor skills. Recently, Liu \etal used affined
feedback laws to parameterize existing controllers
\cite{Liu:2012:TRC}. They further showed that composition of these
parameterized controllers can be done by developing special
controllers for the transition motions.

We demonstrate our framework by training a virtual character to
perform highly dynamic motor skills. Motions with relatively long
airborne time can be physically simulated by designing
state-machine-like controllers
\cite{Hodgins:1995:AHA,Faloutsos:2001:CCF}, sampling around reference
trajectories \cite{Liu:2010:SCM,Liu:2012:TRC}, or planning center of
mass or momentum
\cite{Mordatch:2010:RPL,Ha:2012:FAL}. Alternatively, trajectory optimization under
physics constraints is also a viable approach to produce highly
dynamic motions
\revised{
\cite{Popovic:1999:PBM,Liu:2002:SCD,Fang:2003:ESP,Safonova:2004:SPR,Coros:2011:LSS,Borno:2013:TOF}
}
%% \original{The optimization provides a mathematical way to formalize the energy
%% expenditure or motion style as an objective function, but this
%% approach cannot be applied to interactive environment. }
%% \updated{The optimization provides a mathematical way to formalize the energy
%% expenditure or motion style as an objective function, but the
%% monolithic formulation can be inefficient when user wants to modify the 
%% result motion. 
%% Instead, our approach creates a series of optimization problem
%% using user instructions and solves for the parameters in an efficient manner
%% by exploiting the history of past failures.}
%% \karen{
\revised{
  The optimization provides a mathematical way to formalize the
  energy expenditure or motion style as an objective function, but
  this approach is too costly for consecutive motion editing
  process. In contrast, our approach solves for a series of
  optimization problems in an efficient manner by exploiting the
  accumulated information from the previous problems.
}
%% }

We chose highly dynamic motions to showcase our work, but our
controller design framework is generous to other types of motions as
long as the appropriate control rigs are provided.

\ignorethis{
SUNADA, C., ARGAEZ, D., DUBOWSKY, S., AND MAVROIDIS, C. 1994. A coordinated jacobian transpose control for mo- bile multi-limbed robotic systems. In Proc. IEEE Int’l Conf. on Robotics and Automation, 1910–1915.

- manual design
Hodgins, Faloutsos, Yin 2007, Coros

- optimal control
Muico 2009, da Silva 2008, de Lasa, Mordatch, Wu

- parameter optimization
Wang 2009, 2010, 2012, Yin 2008, Tan 2010, 

- controller generalization

- dynamic motion


- Biped locomotion

* (Simbicon, 2007, Yin), (Interactive, 2008, Da Silva), (Contact-aware, 2009, Muico), 
(Feature-based, 2010, Lasa), (Robust, 2010, Mordatch), (Terrain-adaptive, 2010, Wu),
(Generalize, 2010, Coros), (Locomotion, 2011, Coros)
* Feed back, (Data-driven, 2010, Lee), (Inverted, 2010, Kwon), (Optimal, 2010, Ye)
(Terrain, 2012, Liu)
* Biology inspired, (Bio-inspired, 2012, Wang)

- Dynamic motion
* Manual craft (Animating, 1995, Hodgins), (Simulation of, 1998, Wooten)
* Tracking (Contact, 2010, Liu)
* Interface (User interface, 2005, Zhao)
* Editting (Flipping, 2007, Majkowska), (Editting, 2010, Sok) 
* Space-time (Synthesis, 2002, Liu), (Efficient, 2003, Fang), (Synthesizing, 2004, Safonova),
(Adaptation, 2004, Sulejmansic)
* Momentum and inertia planning (Falling, 2012, Ha)

- Parameter-optimization for controllers
* Motion tracking (Synthesis, 2005, Sharon), (Simulating, 2007, Sok)
* Locomotion(Continuation, 2008, Yin), (Optimizing, 2009, Wang), (Optimizing, 2010, Wang), (Bio-inspired, 2012, Wang)
* Swimming (Articulated, 2010, Tan)


- Compositing and continuating the controllers
* Compositing (Compositing, 2001, Faloutsos)
* Linear Bellman Equation (Linear, 2009, Da Silva)
* Track multiple (Composite, 2011, Muico)
* Task-based, reinforcement learning (Robust, 2009, Coros)
* Feedback (Terrain, 2012, Liu)
}




