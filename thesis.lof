\contentsline {figure}{\numberline {1}{\ignorespaces A falling motion of Parkour.}}{5}
\contentsline {figure}{\numberline {2}{\ignorespaces Hardware of BioloidGP robot.}}{5}
\contentsline {figure}{\numberline {3}{\ignorespaces The proposed learning frame uses human readable instructions to teach motions.}}{6}
\contentsline {figure}{\numberline {4}{\ignorespaces Bongo Board balance toy.}}{7}
\contentsline {figure}{\numberline {5}{\ignorespaces A cat is able to right itself as it falls to land on its feet, irrespective of its initial orientation.}}{13}
\contentsline {figure}{\numberline {6}{\ignorespaces Difference between a real robot and its simulation model results different motions from same controllers.}}{16}
\contentsline {figure}{\numberline {7}{\ignorespaces A simulated character lands on the roof of a car, leaps forward, dive-rolls on the sidewalk, and gets back on its feet, all in one continuous motion.}}{19}
\contentsline {figure}{\numberline {8}{\ignorespaces Three stages in the landing phase.}}{23}
\contentsline {figure}{\numberline {9}{\ignorespaces The left and middle are the desired landing poses for the hands-first strategy and the feet-first strategy, respectively. The right is the ready-to-roll pose for the feet-first strategy, which we track only the upper body. }}{24}
\contentsline {figure}{\numberline {10}{\ignorespaces Samples for hands-first landing strategy. Successful samples are bounded between top and bottom planes along $\theta ^{(T)}$ axis. The middle plane, average of the two, indicates the linear relation of the ideal landing condition. }}{25}
\contentsline {figure}{\numberline {11}{\ignorespaces Samples in the space of $v_z^{(T)}$, $\omega _y^{(T)}$, and $\theta ^{(T)}$. The spinning velocity $\omega _y^{(T)}$ has minimal effect on the success of a sample. }}{26}
\contentsline {figure}{\numberline {12}{\ignorespaces Among 16 poses in $\ensuremath {\mathbf {Q}}$, pose 1, 2, 9, and 13 are frequently selected by the airborne controller }}{29}
\contentsline {figure}{\numberline {13}{\ignorespaces Landing phase controller.}}{30}
\contentsline {figure}{\numberline {14}{\ignorespaces Two-step impact stage for the feet-first strategy.}}{31}
\contentsline {figure}{\numberline {15}{\ignorespaces Hands-first landing motion.}}{34}
\contentsline {figure}{\numberline {16}{\ignorespaces Left: The character model used for most examples. Right: A character with a disproportionately large torso and short legs. }}{37}
\contentsline {figure}{\numberline {17}{\ignorespaces Maximal stress for each joint from a hands-first landing motion. Results are quantitatively similar across all of our simulations. Green: Ragdoll motion. Blue: Our motion. Orange: Joint stress scaled by mass.}}{38}
\contentsline {figure}{\numberline {18}{\ignorespaces Feet-first landing motion.}}{40}
\contentsline {figure}{\numberline {19}{\ignorespaces The abstract model consists of a telescopic inverted pendulum and a massless stopper.}}{45}
\contentsline {figure}{\numberline {20}{\ignorespaces Contact graphs}}{48}
\contentsline {figure}{\numberline {21}{\ignorespaces First row: BioloidGP forward falling from a one-foot stance due to a $5.0$N push. Second row: BioloidGP forward falling from a one-foot stance due to a $8.0$N push. Third row: Atlas forward falling from a two-feet stance due to a $1000$N push. Fourth row: Atlas forward falling from a two-feet stance due to a $2000$N push.}}{53}
\contentsline {figure}{\numberline {22}{\ignorespaces COM trajectories between the abstract model (Blue) and the robot (Red). Top left: BioloidGP forward falling from a one-foot stance due to a $5.0$N push. Top right: BioloidGP forward falling from a one-foot stance due to a $8.0$N push. Bottom left: Atlas forward falling from a two-feet stance due to a $1000$N push. Bottom right: Atlas forward falling from a two-feet stance due to a $2000$N push.}}{55}
\contentsline {figure}{\numberline {23}{\ignorespaces We measured the acceleration at the head of BioloidGP (Left). For both $0.0$N (Middle) and $0.5$N (Right) cases, the planned motions (Red) yielded about 68\% of the maximum acceleration of the unplanned motions (Blue).}}{57}
\contentsline {figure}{\numberline {24}{\ignorespaces Two precision jumps on narrow rails. }}{60}
\contentsline {figure}{\numberline {25}{\ignorespaces Overview diagram. }}{63}
\contentsline {figure}{\numberline {26}{\ignorespaces The user can adjust the positions of hands by giving an instruction ``TRANSLATE hands forward BY $0.5m$''. The instruction will add an IKPose rig for arms and modify the desired position of hands by $0.5m$ in the forward direction. }}{68}
\contentsline {figure}{\numberline {27}{\ignorespaces A comparison of a single SVM and multiple SVMs. Left: Using a single SVM to represent the feasible region (purple triangle), the SVM cannot be activated after eight samples, due to the lack of positive samples. Right: If the feasible regions is represented by the intersection of three constraints, each of which is approximated by an individual SVM, eight samples are sufficient to active two SVMs (shown in green and blue). Dashed lines indicate the current SVM approximation of constraints. }}{75}
\contentsline {figure}{\numberline {28}{\ignorespaces Contour of the trained SVMs for the second toy problem (Table 7\hbox {}). The feasible region classified by SVMs is filled with red. The ground truth feasible region is outlined by the dashed lines. For clarity, the figure is zoomed into the region from $[-5,5]^2$ to $[3, 5]^2$ . }}{78}
\contentsline {figure}{\numberline {29}{\ignorespaces We use these three target poses for all the initial controllers, except for the rolling phase of drop-and-roll. }}{81}
\contentsline {figure}{\numberline {30}{\ignorespaces Monkey vault and wall-backflip. }}{83}
\contentsline {figure}{\numberline {31}{\ignorespaces Framework of our approach.}}{90}
\contentsline {figure}{\numberline {32}{\ignorespaces Direct policy search.}}{91}
\contentsline {figure}{\numberline {33}{\ignorespaces Robot balancing on a bongoboard.}}{97}
\contentsline {figure}{\numberline {34}{\ignorespaces Simulation result of a policy optimized for the Lagrangian model (left column) and Box2D model (right column). In each snapshot, the left and right figures are the Box2D and Lagrangian model simulations respectively.}}{100}
\contentsline {figure}{\numberline {35}{\ignorespaces Velocity field of the learned dynamics model. Cyan: training data; red: prediction; blue: ground truth.}}{101}
\contentsline {figure}{\numberline {36}{\ignorespaces Change of cost function value in Box2D simulations over iterations.}}{102}
\contentsline {figure}{\numberline {37}{\ignorespaces Balancing success rate in Box2D simulation with noise, starting from various initial wheel and board angles. (a) The policy has been optimized with Box2D simulation without noise. (b) The policy has been optimized with Box2D simulation with noise.}}{104}
