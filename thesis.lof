\contentsline {figure}{\numberline {1}{\ignorespaces A falling motion of Parkour.}}{5}
\contentsline {figure}{\numberline {2}{\ignorespaces Hardware of BioloidGP robot.}}{5}
\contentsline {figure}{\numberline {3}{\ignorespaces The proposed learning frame uses human readable instructions to teach motions.}}{6}
\contentsline {figure}{\numberline {4}{\ignorespaces Bongo Board balance toy.}}{7}
\contentsline {figure}{\numberline {5}{\ignorespaces A cat is able to right itself as it falls to land on its feet, irrespective of its initial orientation.}}{13}
\contentsline {figure}{\numberline {6}{\ignorespaces Difference between a real robot and its simulation model results different motions from same controllers.}}{16}
\contentsline {figure}{\numberline {7}{\ignorespaces A simulated character lands on the roof of a car, leaps forward, dive-rolls on the sidewalk, and gets back on its feet, all in one continuous motion.}}{19}
\contentsline {figure}{\numberline {8}{\ignorespaces Three stages in the landing phase.}}{23}
\contentsline {figure}{\numberline {9}{\ignorespaces The left and middle are the desired landing poses for the hands-first strategy and the feet-first strategy, respectively. The right is the ready-to-roll pose for the feet-first strategy, which we track only the upper body. }}{24}
\contentsline {figure}{\numberline {10}{\ignorespaces Samples for hands-first landing strategy. Successful samples are bounded between top and bottom planes along $\theta ^{(T)}$ axis. The middle plane, average of the two, indicates the linear relation of the ideal landing condition. }}{25}
\contentsline {figure}{\numberline {11}{\ignorespaces Samples in the space of $v_z^{(T)}$, $\omega _y^{(T)}$, and $\theta ^{(T)}$. The spinning velocity $\omega _y^{(T)}$ has minimal effect on the success of a sample. }}{26}
\contentsline {figure}{\numberline {12}{\ignorespaces Among 16 poses in $\ensuremath {\mathbf {Q}}$, pose 1, 2, 9, and 13 are frequently selected by the airborne controller }}{29}
\contentsline {figure}{\numberline {13}{\ignorespaces Landing phase controller.}}{30}
\contentsline {figure}{\numberline {14}{\ignorespaces Two-step impact stage for the feet-first strategy.}}{31}
\contentsline {figure}{\numberline {15}{\ignorespaces Hands-first landing motion.}}{34}
\contentsline {figure}{\numberline {16}{\ignorespaces Left: The character model used for most examples. Right: A character with a disproportionately large torso and short legs. }}{37}
\contentsline {figure}{\numberline {17}{\ignorespaces Maximal stress for each joint from a hands-first landing motion. Results are quantitatively similar across all of our simulations. Green: Ragdoll motion. Blue: Our motion. Orange: Joint stress scaled by mass.}}{38}
\contentsline {figure}{\numberline {18}{\ignorespaces Feet-first landing motion.}}{40}
\contentsline {figure}{\numberline {19}{\ignorespaces The abstract model consists of a telescopic inverted pendulum and a massless stopper.}}{45}
\contentsline {figure}{\numberline {20}{\ignorespaces Contact graphs}}{48}
\contentsline {figure}{\numberline {21}{\ignorespaces First row: BioloidGP forward falling from a one-foot stance due to a $5.0$N push. Second row: BioloidGP forward falling from a one-foot stance due to a $8.0$N push. Third row: Atlas forward falling from a two-feet stance due to a $1000$N push. Fourth row: Atlas forward falling from a two-feet stance due to a $2000$N push.}}{53}
\contentsline {figure}{\numberline {22}{\ignorespaces COM trajectories between the abstract model (Blue) and the robot (Red). Top left: BioloidGP forward falling from a one-foot stance due to a $5.0$N push. Top right: BioloidGP forward falling from a one-foot stance due to a $8.0$N push. Bottom left: Atlas forward falling from a two-feet stance due to a $1000$N push. Bottom right: Atlas forward falling from a two-feet stance due to a $2000$N push.}}{55}
\contentsline {figure}{\numberline {23}{\ignorespaces We measured the acceleration at the head of BioloidGP (Left). For both $0.0$N (Middle) and $0.5$N (Right) cases, the planned motions (Red) yielded about 68\% of the maximum acceleration of the unplanned motions (Blue).}}{57}
\contentsline {figure}{\numberline {24}{\ignorespaces Framework of our approach.}}{61}
\contentsline {figure}{\numberline {25}{\ignorespaces Direct policy search.}}{62}
\contentsline {figure}{\numberline {26}{\ignorespaces Robot balancing on a bongoboard.}}{69}
\contentsline {figure}{\numberline {27}{\ignorespaces Simulation result of a policy optimized for the Lagrangian model (left column) and Box2D model (right column). In each snapshot, the left and right figures are the Box2D and Lagrangian model simulations respectively.}}{71}
\contentsline {figure}{\numberline {28}{\ignorespaces Velocity field of the learned dynamics model. Cyan: training data; red: prediction; blue: ground truth.}}{72}
\contentsline {figure}{\numberline {29}{\ignorespaces Change of cost function value in Box2D simulations over iterations.}}{73}
\contentsline {figure}{\numberline {30}{\ignorespaces Balancing success rate in Box2D simulation with noise, starting from various initial wheel and board angles. (a) The policy has been optimized with Box2D simulation without noise. (b) The policy has been optimized with Box2D simulation with noise.}}{75}
