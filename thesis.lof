\contentsline {figure}{\numberline {1}{\ignorespaces A fall of a virtual character.}}{4}
\contentsline {figure}{\numberline {2}{\ignorespaces A two-step falling strategy of a humanoid robot}}{5}
\contentsline {figure}{\numberline {3}{\ignorespaces A challenging stunt that a character jumps twice and rolls on the ground.}}{6}
\contentsline {figure}{\numberline {4}{\ignorespaces A simple legged robot on the bongoboard.}}{8}
\contentsline {figure}{\numberline {5}{\ignorespaces A cat is able to right itself as it falls to land on its feet, irrespective of its initial orientation.}}{14}
\contentsline {figure}{\numberline {6}{\ignorespaces Difference between a real robot and its simulation model results different motions from same controllers.}}{17}
\contentsline {figure}{\numberline {7}{\ignorespaces A simulated character lands on the roof of a car, leaps forward, dive-rolls on the sidewalk, and gets back on its feet, all in one continuous motion.}}{20}
\contentsline {figure}{\numberline {8}{\ignorespaces Three stages in the landing phase.}}{24}
\contentsline {figure}{\numberline {9}{\ignorespaces The left and middle are the desired landing poses for the hands-first strategy and the feet-first strategy, respectively. The right is the ready-to-roll pose for the feet-first strategy, which we track only the upper body. }}{25}
\contentsline {figure}{\numberline {10}{\ignorespaces Samples for hands-first landing strategy. Successful samples are bounded between top and bottom planes along $\theta ^{(T)}$ axis. The middle plane, average of the two, indicates the linear relation of the ideal landing condition. }}{26}
\contentsline {figure}{\numberline {11}{\ignorespaces Samples in the space of $v_z^{(T)}$, $\omega _y^{(T)}$, and $\theta ^{(T)}$. The spinning velocity $\omega _y^{(T)}$ has minimal effect on the success of a sample. }}{27}
\contentsline {figure}{\numberline {12}{\ignorespaces Among 16 poses in $\ensuremath {\mathbf {Q}}$, pose 1, 2, 9, and 13 are frequently selected by the airborne controller }}{30}
\contentsline {figure}{\numberline {13}{\ignorespaces Landing phase controller.}}{31}
\contentsline {figure}{\numberline {14}{\ignorespaces Two-step impact stage for the feet-first strategy.}}{32}
\contentsline {figure}{\numberline {15}{\ignorespaces Hands-first landing motion.}}{35}
\contentsline {figure}{\numberline {16}{\ignorespaces Left: The character model used for most examples. Right: A character with a disproportionately large torso and short legs. }}{38}
\contentsline {figure}{\numberline {17}{\ignorespaces Maximal stress for each joint from a hands-first landing motion. Results are quantitatively similar across all of our simulations. Green: Ragdoll motion. Blue: Our motion. Orange: Joint stress scaled by mass.}}{39}
\contentsline {figure}{\numberline {18}{\ignorespaces Feet-first landing motion.}}{41}
\contentsline {figure}{\numberline {19}{\ignorespaces The abstract model consists of a telescopic inverted pendulum and a massless stopper.}}{46}
\contentsline {figure}{\numberline {20}{\ignorespaces Contact graphs}}{49}
\contentsline {figure}{\numberline {21}{\ignorespaces First row: BioloidGP forward falling from a one-foot stance due to a $5.0$N push. Second row: BioloidGP forward falling from a one-foot stance due to a $8.0$N push. Third row: Atlas forward falling from a two-feet stance due to a $1000$N push. Fourth row: Atlas forward falling from a two-feet stance due to a $2000$N push.}}{54}
\contentsline {figure}{\numberline {22}{\ignorespaces COM trajectories between the abstract model (Blue) and the robot (Red). Top left: BioloidGP forward falling from a one-foot stance due to a $5.0$N push. Top right: BioloidGP forward falling from a one-foot stance due to a $8.0$N push. Bottom left: Atlas forward falling from a two-feet stance due to a $1000$N push. Bottom right: Atlas forward falling from a two-feet stance due to a $2000$N push.}}{56}
\contentsline {figure}{\numberline {23}{\ignorespaces We measured the acceleration at the head of BioloidGP (Left). For both $0.0$N (Middle) and $0.5$N (Right) cases, the planned motions (Red) yielded about 68\% of the maximum acceleration of the unplanned motions (Blue).}}{58}
\contentsline {figure}{\numberline {24}{\ignorespaces Two precision jumps on narrow rails. }}{61}
\contentsline {figure}{\numberline {25}{\ignorespaces Overview diagram. }}{64}
\contentsline {figure}{\numberline {26}{\ignorespaces The user can adjust the positions of hands by giving an instruction ``TRANSLATE hands forward BY $0.5m$''. The instruction will add an IKPose rig for arms and modify the desired position of hands by $0.5m$ in the forward direction. }}{69}
\contentsline {figure}{\numberline {27}{\ignorespaces A comparison of a single SVM and multiple SVMs. Left: Using a single SVM to represent the feasible region (purple triangle), the SVM cannot be activated after eight samples, due to the lack of positive samples. Right: If the feasible regions is represented by the intersection of three constraints, each of which is approximated by an individual SVM, eight samples are sufficient to active two SVMs (shown in green and blue). Dashed lines indicate the current SVM approximation of constraints. }}{76}
\contentsline {figure}{\numberline {28}{\ignorespaces Contour of the trained SVMs for the second toy problem (Table 7\hbox {}). The feasible region classified by SVMs is filled with red. The ground truth feasible region is outlined by the dashed lines. For clarity, the figure is zoomed into the region from $[-5,5]^2$ to $[3, 5]^2$ . }}{79}
\contentsline {figure}{\numberline {29}{\ignorespaces We use these three target poses for all the initial controllers, except for the rolling phase of drop-and-roll. }}{82}
\contentsline {figure}{\numberline {30}{\ignorespaces Monkey vault and wall-backflip. }}{84}
\contentsline {figure}{\numberline {31}{\ignorespaces Top: Vertical jump with parameterized target height, $3cm$ to $8cm$ ($8cm$ is shown). Middle: Kick with parameterized target distance, $0.3m$ to $0.6m$ ($0.6m$ is shown). Bottom: Walk with parameterized target speed, $6.7cm/s$ to $13.3cm/s$ ($13.3cm/s$ is shown). }}{101}
\contentsline {figure}{\numberline {32}{\ignorespaces Comparison on three parameterized control problems. The cost (Equation\nobreakspace {}(38\hbox {})) is computed by averaging seven optimization trials. In all problems, our algorithm converges faster than CMA-ES, especially when the parameterized skill function is of cubic form.}}{103}
\contentsline {figure}{\numberline {33}{\ignorespaces The impact of task discretization on convergence. More discrete tasks slow down the convergence of CMA-ES significantly, while it has negligible impact on our algorithm.}}{106}
\contentsline {figure}{\numberline {34}{\ignorespaces Comparison between our algorithm and the individual learning approach. The quality of the low-resolution policy (shown in green) is comparable with the high-resolution one (shown in blue) for those six tasks used for training (dotted vertical lines). However, for those tasks corresponding to interpolated policy parameters, there is a significant discrepancy between the quality of low-resolution and high-resolution policies. In contrast, our policy (shown in red) learned with only six tasks (M = 6) is comparable to the high-resolution one.}}{108}
\contentsline {figure}{\numberline {35}{\ignorespaces BioloidGP hardware. }}{109}
\contentsline {figure}{\numberline {36}{\ignorespaces Framework of our approach.}}{113}
\contentsline {figure}{\numberline {37}{\ignorespaces Direct policy search.}}{114}
\contentsline {figure}{\numberline {38}{\ignorespaces Robot balancing on a bongoboard.}}{120}
\contentsline {figure}{\numberline {39}{\ignorespaces Simulation result of a policy optimized for the Lagrangian model (left column) and Box2D model (right column). In each snapshot, the left and right figures are the Box2D and Lagrangian model simulations respectively.}}{123}
\contentsline {figure}{\numberline {40}{\ignorespaces Velocity field of the learned dynamics model. Cyan: training data; red: prediction; blue: ground truth.}}{124}
\contentsline {figure}{\numberline {41}{\ignorespaces Change of cost function value in Box2D simulations over iterations.}}{125}
\contentsline {figure}{\numberline {42}{\ignorespaces Balancing success rate in Box2D simulation with noise, starting from various initial wheel and board angles. (a) The policy has been optimized with Box2D simulation without noise. (b) The policy has been optimized with Box2D simulation with noise.}}{127}
